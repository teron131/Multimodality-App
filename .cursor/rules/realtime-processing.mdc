---
description: 
globs: 
alwaysApply: true
---
# Real-time Processing Standards

## 🎯 Real-time Goal
Deliver **Gemini-class real-time multimodal experiences** through **OpenAI-compatible WebSocket APIs** for live audio, video, and interactive processing.

## 🌐 WebSocket Architecture - [realtime.py](mdc:multimodality_app/routes/realtime.py)

### Core Real-time Capabilities
- **Live Audio Transcription**: Streaming speech-to-text with immediate results
- **Real-time Image Analysis**: Camera feed processing with instant feedback  
- **Interactive Conversations**: Multi-turn chat with media context
- **Collaborative Processing**: Multiple users, shared sessions

### OpenAI Realtime API Compatibility
Follow OpenAI's real-time API patterns for seamless integration:

```python
# WebSocket message format (OpenAI-compatible)
{
    "type": "conversation.item.create",
    "item": {
        "type": "message",
        "role": "user", 
        "content": [
            {"type": "input_audio", "audio": "base64_audio_data"},
            {"type": "input_text", "text": "Analyze this audio"}
        ]
    }
}

# Response format
{
    "type": "response.audio_transcript.delta",
    "delta": "partial transcription text...",
    "item_id": "msg_001"
}
```

## 🎵 Streaming Audio Processing

### Audio Stream Handler
```python
import asyncio
from typing import AsyncIterator

class AudioStreamProcessor:
    """Real-time audio processing with buffering."""
    
    def __init__(self, chunk_size: int = 1024):
        self.chunk_size = chunk_size
        self.buffer = bytearray()
        
    async def process_stream(self, audio_chunks: AsyncIterator[bytes]) -> AsyncIterator[dict]:
        """Process streaming audio with real-time transcription."""
        async for chunk in audio_chunks:
            self.buffer.extend(chunk)
            
            # Process when buffer reaches minimum size
            if len(self.buffer) >= self.chunk_size:
                result = await self.transcribe_chunk(self.buffer[:self.chunk_size])
                self.buffer = self.buffer[self.chunk_size:]
                yield result
```

### WebSocket Audio Endpoint
```python
@app.websocket("/v1/realtime/audio")
async def audio_realtime(websocket: WebSocket):
    """OpenAI-compatible real-time audio processing."""
    await websocket.accept()
    processor = AudioStreamProcessor()
    
    try:
        async for message in websocket.iter_text():
            data = json.loads(message)
            
            if data["type"] == "audio.delta":
                # Process audio chunk
                audio_bytes = base64.b64decode(data["audio"])
                result = await processor.process_chunk(audio_bytes)
                
                # Send OpenAI-format response
                await websocket.send_json({
                    "type": "conversation.item.created",
                    "item": {
                        "type": "message",
                        "role": "assistant",
                        "content": result["text"]
                    }
                })
                
    except WebSocketDisconnect:
        logger.info("Audio stream disconnected")
```

## 📷 Real-time Vision Processing

### Camera Feed Analysis
```python
@app.websocket("/v1/realtime/vision") 
async def vision_realtime(websocket: WebSocket):
    """Real-time image analysis from camera feeds."""
    await websocket.accept()
    
    try:
        async for message in websocket.iter_bytes():
            # Expect JPEG frame data
            image_analysis = await analyze_image_realtime(message)
            
            await websocket.send_json({
                "type": "vision.analysis",
                "timestamp": time.time(),
                "analysis": image_analysis,
                "confidence": image_analysis.get("confidence", 0.0)
            })
            
    except WebSocketDisconnect:
        logger.info("Vision stream disconnected")
```

### Frame Processing Strategy
```python
async def analyze_image_realtime(image_bytes: bytes) -> dict:
    """Optimized real-time image analysis."""
    # Resize for speed while maintaining quality
    image = await resize_for_processing(image_bytes, max_size=512)
    
    # Use fast processing models for real-time
    result = await get_provider().vision_analysis_fast(image, 
        prompt="Describe what you see briefly"
    )
    
    return {
        "description": result["description"],
        "objects": result.get("objects", []),
        "confidence": result.get("confidence", 0.0),
        "processing_time": result.get("duration", 0.0)
    }
```

## 🎪 Interactive Multimodal Sessions

### Session Management
```python
class RealtimeSession:
    """Manage stateful real-time interactions."""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.conversation_history = []
        self.active_media = {}
        self.websocket_connections = set()
        
    async def add_message(self, content: dict):
        """Add message to session with media context."""
        self.conversation_history.append({
            "timestamp": time.time(),
            "content": content,
            "media_context": self.active_media.copy()
        })
        
    async def broadcast_update(self, update: dict):
        """Send update to all connected clients."""
        disconnected = []
        for ws in self.websocket_connections:
            try:
                await ws.send_json(update)
            except:
                disconnected.append(ws)
        
        # Clean up disconnected clients
        for ws in disconnected:
            self.websocket_connections.discard(ws)
```

### Multi-client Coordination
```python
# Global session manager
sessions: Dict[str, RealtimeSession] = {}

@app.websocket("/v1/realtime/session/{session_id}")
async def join_realtime_session(websocket: WebSocket, session_id: str):
    """Join or create real-time collaborative session."""
    await websocket.accept()
    
    # Get or create session
    if session_id not in sessions:
        sessions[session_id] = RealtimeSession(session_id)
    
    session = sessions[session_id]
    session.websocket_connections.add(websocket)
    
    try:
        # Send session history to new client
        await websocket.send_json({
            "type": "session.history",
            "history": session.conversation_history[-10:]  # Last 10 messages
        })
        
        # Handle real-time messages
        async for message in websocket.iter_text():
            data = json.loads(message)
            await session.add_message(data)
            
            # Process and broadcast to all clients
            result = await process_realtime_message(data)
            await session.broadcast_update(result)
            
    except WebSocketDisconnect:
        session.websocket_connections.discard(websocket)
        logger.info(f"Client disconnected from session {session_id}")
```

## ⚡ Performance Optimization

### Streaming Response Patterns
```python
async def stream_llm_response(prompt: str, model: str) -> AsyncIterator[dict]:
    """Stream LLM responses for real-time interaction."""
    async for chunk in get_provider().stream_completion(prompt, model):
        yield {
            "type": "response.text.delta", 
            "delta": chunk.get("text", ""),
            "done": chunk.get("finished", False)
        }
```

### Buffer Management
```python
class StreamBuffer:
    """Efficient streaming buffer for real-time processing."""
    
    def __init__(self, max_size: int = 8192):
        self.max_size = max_size
        self.buffer = collections.deque(maxlen=max_size)
        
    async def add_chunk(self, data: bytes):
        """Add data chunk with automatic overflow handling."""
        self.buffer.append(data)
        
        # Process if buffer is full
        if len(self.buffer) >= self.max_size:
            return await self.process_buffer()
        return None
```

## 🔧 Integration with Media Processing

### Real-time Audio Integration
Route streaming audio through [audio.py](mdc:multimodality_app/media_processing/audio.py):
```python
from ..media_processing.audio import AudioProcessor

async def process_realtime_audio(audio_chunk: bytes) -> dict:
    """Process audio chunk with real-time optimizations."""
    processor = AudioProcessor(realtime_mode=True)
    return await processor.transcribe_streaming(audio_chunk)
```

### Real-time Image Integration  
Route camera frames through [image.py](mdc:multimodality_app/media_processing/image.py):
```python
from ..media_processing.image import ImageProcessor

async def process_realtime_image(image_bytes: bytes) -> dict:
    """Process image with real-time optimizations."""
    processor = ImageProcessor(fast_mode=True)
    return await processor.analyze_streaming(image_bytes)
```

## 📊 Real-time Monitoring

### Connection Health
```python
async def monitor_websocket_health():
    """Monitor WebSocket connection health."""
    active_connections = sum(len(session.websocket_connections) 
                           for session in sessions.values())
    
    logger.info(f"Active WebSocket connections: {active_connections}")
    
    # Clean up dead sessions
    dead_sessions = [sid for sid, session in sessions.items() 
                    if not session.websocket_connections]
    
    for sid in dead_sessions:
        del sessions[sid]
```

### Performance Metrics
```python
async def log_realtime_metrics(operation: str, duration: float):
    """Track real-time processing performance."""
    logger.info(f"Realtime {operation}: {duration:.3f}s")
    
    # Alert if processing is too slow
    if duration > 0.1:  # 100ms threshold
        logger.warning(f"Slow realtime processing: {operation} took {duration:.3f}s")
```

## 🚀 Advanced Features

### Voice Activity Detection
```python
async def detect_speech_activity(audio_chunk: bytes) -> bool:
    """Detect when user is speaking for smart processing."""
    # Implement VAD logic
    energy = calculate_audio_energy(audio_chunk)
    return energy > SPEECH_THRESHOLD
```

### Contextual Processing
```python
async def process_with_context(new_input: dict, session: RealtimeSession) -> dict:
    """Process input with session context for better results."""
    context = {
        "recent_messages": session.conversation_history[-5:],
        "active_media": session.active_media,
        "user_preferences": session.get_user_preferences()
    }
    
    return await get_provider().process_contextual(new_input, context)
```
